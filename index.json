[{"content":"This section covers the basic information needed for copying streams from a Source to a Target.\nPlacement Architecture Generally it is best to place the Replicator nearest in terms of network latency to the Target.\nWhen sampling data or tracking and publishing advisories it should be placed nearest to the Source. Advisories are also only Published to the Source so we’d want to give those the best chance of success possible by eliminating network issues.\nSee details in each deployment scenario for further details and placement constraints.\nRequirements Both the Source and Target Stream has to exist before this tool will run. The 2 streams do not need to have the same configuration, for example one can keep data for an hour the other for a month. Or one can be Memory based while the other File.\nIf the Source does not exist the Replicator will keep trying until it does and then start. If the Target does not exist once the Source is found the Target will be created using the same configuration as the Source.\nFor now, we assume like-for-like streams in 2 separate servers.\nBasic Configuration The Stream Replicator itself has some configuration to set, this is going to be the same for all modes of copy.\nname: US-EAST monitor_port: 8080 loglevel: info state_store: /var/lib/stream-replicator logfile: /var/log/stream-replicator.log streams: [] # see other documentation pages This is fairly self-explanatory except for the name property: typically one would deploy a tree-like structure of many Streams replicated into one, perhaps many data centers all having node events.\nTo assist with configuration all of these locations would have the same Stream name and same subjects. But centrally one might want to know where a specific message is from.\nSetting a unique per-location name like US-EAST will add a header to every copied message that will include this name.\nLoglevels can be debug, info (default) or warn and if monitor_port is not set (default) Prometheus metrics will not be exposed.\nThe remaining settings is obvious and match what is in the RPM packages.\nTLS TLS is supported, one can have per Target or Source settings. Per Stream settings or per Replicator settings. The most specific will be used for example, given this partial configuration file:\ntls:  ca: /path/to/ca.pem  cert: /path/to/cert.pem  key: /path/to/key.pem streams:  - first:  target_tls:  ca: /path/to/first-ca.pem  cert: /path/to/first-cert.pem  key: /path/to/first-key.pem  - second:  target_tls:  ca: /path/to/second-ca.pem  cert: /path/to/second-cert.pem  key: /path/to/second-key.pem  Here we have a Replicator-wide default referencing ca.pem, cert.pem and key.pem. All Sources will use this, all Targets without target_tls will use this. But those targets with specific ones will use those.\nThis is based on the assumption that one might copy many data from 1 single JetStream server to many locations. This model allows one to state the source TLS once only for all streams.\nSource specific TLS can be set with source_tls. At a Stream level one can also set tls to have the same TLS settings used for Source and Target.\nChoria JWT Tokens Choria Broker supports running in a mode that requires Choria specific JWT tokens and private keys in order to connect to it. Replicator supports these. One can have per Target or Source settings. Per Stream settings or per Replicator settings. The most specific will be used for example, given this partial configuration file:\nchoria:  seed_file: /etc/stream-replicator/choria.seed  jwt_file: /etc/stream-replicator/choria.jwt  collective: choria streams:  - first:  target_choria:  seed_file: /etc/stream-replicator/first.seed  jwt_file: /etc/stream-replicator/first.jwt  collective: choria  - second:  target_choria:  seed_file: /etc/stream-replicator/second.seed  jwt_file: /etc/stream-replicator/second.jwt  collective: choria  Here we have a Replicator-wide default referencing choria.seed, choria.jwt and choria collective. All Sources will use this, all Targets without target_tls will use this. But those targets with specific ones will use those.\nThis is based on the assumption that one might copy many data from 1 single JetStream server to many locations. This model allows one to state the source connection details once only for all streams.\nSource specific Choria connection details can be set with source_choria. At a Stream level one can also set choria to have the same TLS settings used for Source and Target.\n","description":"","tags":null,"title":"Basic Configuration","uri":"/configuration/basic/index.html"},{"content":"We distribute an RPM or Docker container for the Stream Replicator package. Debian is not supported at present.\nYUM Repository For RPMs we publish releases but also nightly builds to our repositories.\nUsers of our Puppet modules will already have these repositories available.\nRelease [choria_release] name=Choria Orchestrator Releases mirrorlist=http://mirrorlists.choria.io/yum/release/el/$releasever/$basearch.txt enabled=True gpgcheck=True repo_gpgcheck=True gpgkey=https://choria.io/RELEASE-GPG-KEY metadata_expire=300 sslcacert=/etc/pki/tls/certs/ca-bundle.crt sslverify=True Nightly Nightly releases are named and versioned stream-replicator-0.99.0.20221109-1.el7.x86_64.rpm where the last part of the version is the date.\n[choria_nightly] name=Choria Orchestrator Nightly mirrorlist=http://mirrorlists.choria.io//yum/nightly/el/$releasever/$basearch.txt enabled=True gpgcheck=True repo_gpgcheck=True gpgkey=https://choria.io/NIGHTLY-GPG-KEY metadata_expire=300 sslcacert=/etc/pki/tls/certs/ca-bundle.crt sslverify=True Docker There is a docker container choria-io/stream-replicator that has releases only.\n","description":"","tags":null,"title":"Installation","uri":"/installation/index.html"},{"content":"Configuring the Replicator requires having credentials and stream details for both the Source and Target streams.\nIf you just want to duplicate/combine streams in the same NATS Server Cluster you should probably use a JetStream Source which Choria Broker also supports.\nReview the sub-sections for scenario specific details.\n","description":"","tags":null,"title":"Configuration","uri":"/configuration/index.html"},{"content":"The most common configuration is simply to copy all messages from a Source to a Target.\nWe won’t show the overall replication configuration, see Basic Configuration for an intro.\nConfiguring the Stream One can configure multiple streams and a Replicator will be configured for each Stream. Today the only mode we support is single worker, order preserving, copy from one Stream to another.\nSources and Targets A Source is where the messages are and a Target is where they are being copied.\nReplicator Placement Generally for best performance when copying all data like this it is best to place the Replicator in the target and setting target_initiated: true. If instead you cannot deploy it in the target you can set this to false (or don’t set it) which would result in a push like behavior.\n Warning While it will function to copy data with the replicator in the Source it is very exposed to latency and can be extremely slow over long links.\n  Copying Entire Streams The settings below will copy NODE_DATA from nats.us-east.example.net to nats.central.example.net, both streams should exist, and it must be called NODE_DATA on both sides.\nstreams:  - stream: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  target_initiated: true Copying to differently named Streams The target stream name can be set to a different name than the source.\nstreams:  - stream: NODE_DATA  target_stream: FLEET_NODES  target_subject_prefix: FLEET_NODES  target_subject_remove: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  target_initiated: true We also show the optional target_subject_prefix and target_subject_remove settings. This will prepend the prefix to the subjects in the source stream and remove the duplication. So if the source was NODE_DATA.host.example.net then the subjects in the target would be FLEET_NODES.host.example.net.\nSetting initial starting location One might want to avoid copying the entire stream from Source to Target, especially when first setting up replication between existing locations.\nWe support the following settings to influence starting point:\n   Setting Description Example     start_sequence A specific message sequence to start at in the Source stream 1024   start_time A specific start time to start at in the Source stream, has to be RFC3339 format 2006-01-02T15:04:05Z07:00   start_delta Calculates a relative start time using this delta, supports h, d, w, M and Y units 1w   start_at_end Sends the next message that arrives as the first one true    Skipping old messages While setting the initial starting location can let you avoid old data at initial start, later if the replicator is down for a while you might find you are traversing ancient data while catching up.\nTo avoid replicating old data you can set a Maximum Age using the max_age: 1h property, to always in all circumstances skip old messages from the source stream.\nFiltering the Source If your source stream has subjects like fleet.REGION.COUNTRY.CITY.\u003e and you want to copy only the Paris related data into your Target you can do so by setting a filter subject like filter_subject: fleet.eu.fr.cdg.\u003e. The target stream will now have all the matching only those subjects, ideal for branch scenarios.\nstreams:  - stream: NODE_DATA  target_stream: FLEET_NODES  target_subject_prefix: FLEET_NODES  target_subject_remove: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  target_initiated: true  filter_subject: sku.eu.fr.cdg.\u003e  start_sequence: 1 ","description":"","tags":null,"title":"Copying All Data","uri":"/configuration/all/index.html"},{"content":"Overview It’s often a case that data in a specific location is produced frequently for maximal correctness of local data sources but in an archive or system that calls into those locations data freshness is not the most important property.\nStream Replicator supports sampling data from the Source and only sending a subset of data to the Target which can greatly improve the efficiency wrt network use and resources in the central archive.\nWe show JSON payload based sampling below, however, since version 0.0.6 we can also sample on Header values and Subjects or parts of Subjects.\nGiven data like this sent from every node in the fleet at a 5 minute interval:\n{  \"sender\":\"some.host.example.net\",  \"metadata\": {} } We want to copy the message to target only when:\n some.host.example.net was seen for the first time If the payload changes significantly in size Once per hour  We optionally want to raise advisories when:\n A new advisory the first time some.host.example.net was seen Should some.host.example.net not be seen for 12 minutes send a timeout warning advisory Should some.host.example.net not be seen for over an hour send an expire advisory and treat it again as new should it return Should some.host.example.net return between expire and timeout send a recovery advisory and resume normal hourly samples  Together this allows the central archive to have fresh data for any new node or node that started gathering metadata after being new and a near real time view of availability of any specific node with enough information to maintain it’s local cache of data.\nYou can combine this feature with age limits.\nConfiguration We will start with the configuration from Copying all messages and we suggest you read that section first.\nstreams:  - stream: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222 Sampling Let’s configure the Replicator to start sampling the above configuration on the sender field in the payload:\nstreams:  - stream: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  inspect_field: sender  inspect_duration: 1h  warn_duration: 12m  size_trigger: 1024  advisory:  subject: NODE_DATA_ADVISORIES  reliable: true This configures the Replicator as described above:\n   Setting Description     inspect_field A JSON key to extract from the payload and key inspection off that. Data without this key will always be copied   inspect_header Inspects the value of a Header in the NATS message. Since version 0.0.6   inspect_subject_token Inspects based on a specific token in the subject or the full subject when set to -1. Since version 0.0.6   inspect_duration The sample frequency and also the longest time we will keep awareness of this node, data will be copied hourly   warn_duration Warn using an advisory when a node was not seen for 12 minutes   size_trigger Should the payload grow or shrink by this many bytes trigger an immediate copy    The size_trigger accommodates the typical scenario where full metadata might be gathered some time after a node starts, we’d send initial minimal metadata and, once gathered, a full set. The first full set message will be copied even if it comes before an hour has passed.\nPick the inspect_duration based on your needs but ensure that it is longer than frequency the nodes will publish data at else all data will be copies.\n Tip The advisory subject can have %s in it that will be replaced with the event type (like timeout) and a %v that will be replaced with the value being tracked. Use this to partition the advisories or to help searching a large store of them\n  We configure advisories that will inform us about statusses of data, advisories will be published to a Stream with the subject NODE_DATA_ADVISORIES and they will be retried a few times should they fail. See Sampling Advisories for details about advisories.\n","description":"","tags":null,"title":"Copying Samples of Data","uri":"/configuration/sampling/index.html"},{"content":"Overview In all scenarios the Replicator supports running in highly available clustered modes.\nGenerally the replicator is designed to be strictly ordered, this means having active-active workers on a single stream is not possible without losing ordered messages. In this scenario we support partitioning streams and scaling out the copier workers horizontally and vertically with HA failover between nodes.\nCombined this allows a very reliable and high performant replication infrastructure to be built.\nLeader Election based Failover The most basic HA configuration is to let multiple instances of the same stream replication profile failover between the instances managing that replication.\nTo perform election a NATS Key-Value bucket called CHORIA_LEADER_ELECTION must be configured. If you are using Choria Streams this will already exist. Else it needs to be made:\n$ nats kv add CHORIA_LEADER_ELECTION --ttl 10s --replicas 3   Note We strongly suggest that this bucket is made on a highly available NATS cluster with at least 3 nodes.\n  Now when configuring the replication configuration for a stream we can set it to use this election bucket to campaign for a leadership:\nstreams:  - stream: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  target_initiated: true  leader_election_name: NODE_DATA With this in place you can simply start any number of replicators and they will elect a leader who will own copying the data. Should that leader fail another one will step in after roughly 30 seconds.\nMessage Partitioning The previous section showed how Leader Election can be used to pick a single node to replicate the data it does mean one node has to do all the copying.\nWe can employ Subject Partitioning to split our work into partitions and then run a replicator per partition. With each partition being highly available. This way we can have multiple active workers all running in a HA cluster.\nIf we assume we have server metadata published to NODE_DATA.node1.example.net we could configure the NATS Servers to perform mapping such that based on node1 in the FQDN a deterministic partition is picked. This will mean that every message for node1.example.net will always end up in the same partition and so ordered replication will keep things valid on that dimension.\nLet’s look how we would partition that data into 2 partitions, we’ll show how to do it in either NATS Server or Choria Streams:\nNATS Configuration:\naccounts { \"test\": { \"mappings\":{ \"NODE_DATA.*.\u003e\": \"NODES.US-EAST-1.{{partition(2,1)}}.{{wildcard(1)}}.\u003e\" } } } Choria Configuration:\nplugin.choria.network.mapping.names = nodes plugin.choria.network.mappin.nodes.source = NODE_DATA.*.\u003e plugin.choria.network.mappin.nodes.destination = NODES.US-EAST-1.{{partition(2,1)}}.{{wildcard(1)}}.\u003e Let’s see what that will do to our subjects.\n$ nats server mappings 'NODE_DATA.*.\u003e' 'NODES.US-EAST-1.{{partition(2,1)}}.{{wildcard(1)}}.\u003e' Enter subjects to test, empty subject terminates. ? Subject NODE_DATA.node3.example.net NODES.US-EAST-1.0.node3.example.net ? Subject NODE_DATA.node1.example.net NODES.US-EAST-1.0.node1.example.net ? Subject NODE_DATA.node2.example.net NODES.US-EAST-1.1.node2.example.net ? Subject NODE_DATA.node3.example.net NODES.US-EAST-1.0.node3.example.net We can see that the messages are mapped from NODE_DATA.\u003cFQDN\u003e\u003e to NODES.US-EAST-1.\u003cPARTITION\u003e.\u003cFQDN\u003e and further that the same node always land in the same partition (see node3.example.net).\nWe would now either create 2 streams, one per partition, or one stream that reads NODES.US-EAST-1.\u003e.\n Note Take care not to overlap the subjects, this is why we rewrite NODE_DATA.\u003e to NODES.\u003e.\n  From this point we simply create a normal leader elected stream configuration that has a subject filter.\nstreams:  - stream: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  target_initiated: true  leader_election_name: NODE_DATA_P0  filter_subject: NODES.US-EAST-1.0.\u003e   - stream: NODE_DATA  source_url: nats://nats.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  target_initiated: true  leader_election_name: NODE_DATA_P1  filter_subject: NODES.US-EAST-1.1.\u003e So we created 2 stream replication profiles that each copy only 1 partition. We set unique leader election names per partition.\nSo end result is that if this replicator runs over 2 nodes the work can spread out over the 2 nodes, fail over, and scale horizontally.\nHA for Sampling When Copying Samples of Data the configuration that needs to be done is identical to the setups before. You can combine partitioned and failover with sampling without a problem.\nWhen sampling is active the Replicators will share state within their cluster using a gossip protocol automatically.\nReview the Monitoring Reference for detail on how to view the gossip messages and to compare the shared state between replicator instances.\n","description":"","tags":null,"title":"HA Clustering","uri":"/configuration/clustering/index.html"},{"content":"Message Source Information Every message that gets copied has a header added called Choria-SR-Source with an example being CHORIA_REGISTRATION 10631 US-EAST-1 PARTITION1 1673433286805. Using this you can determine where any copied message is from, which worker copied it and the delay between the message creation, and it’s arriving in the target.\nFrom this we know the following:\n The source stream is called CHORIA_REGISTRATION The message we are looking at has a Stream Sequence of 10631 in the source stream The replicator name (configured using top level name in the config file) is US-EAST-1 The name set on the stream configuration is PARTITION1 this allows you to tell which specific copier config copied it, helping to identify ordering or partition logic issues The message was stored in the source at 1673433286805 which is a milliseconds since Unix epoch. Delta between this and the message creation time in the target is how long copy took  Sampling Advisories Advisories are created for sampled streams as described in Copying Samples of Data. Read on for full detail about those advisories.\nAdvisories are published to the NATS Server hosting the Source Stream. These advisories could be stored in a Stream and that Stream can also be replicated elsewhere - allowing a large shared real time view of an entire multi location fleet to be build.\nWhen Advisories are published to a Stream one can configure them to be Reliable meaning each message will be tried 10 times on a Backoff policy. Still does not ensure they are 100% reliable but does make them weather short outages.\nAdvisories should therefor not be the only way you use to calculate expiring nodes but to augment another system giving it an insight it could not otherwise have if it was build using Sampled data.\nAdvisory Schema Every advisory message has a protocol key with the value io.choria.sr.v2.age_advisory. This indicates the schema for this message is https://choria.io/schemas/sr/v2/age_advisory.json.\nAdvisory Details Each advisory looks like this:\n{  \"protocol\": \"io.choria.sr.v2.age_advisory\",  \"event_id\": \"26QSY17sb9aJL6UPPG6MLOVjIrE\",  \"inspect_field\": \"sender\",  \"age\": 3600,  \"seen\": 1647351430,  \"replicator\": \"US-EAST\",  \"timestamp\": 1647355030,  \"event\": \"timeout\",  \"value\": \"some.host.example.net\" }    Field Description     protocol Constant indicating the schema   event_id A unique correlation ID per event, k-sortable   inspect_field The key being inspected in the data that caused the advisory   age The time since this sender last checked in, seconds   seen Unix time in UTC this sender was last seen   replicator A unique indicator of the stream replicator where this advisory originated from. name in the main config section   timestamp The time this advisory was generated. Unix timestamp in UTC   event The type of advisory. new, timeout, recover or expire   value The value found in the sent data that identifies the unique sender    Inspecting Run-time Data The stream-replicator command comes with a number of tools to inspect the state and behavior of the system. Most of these will only make sense when deployed clustered or when data sampling is enabled.\n Tip Some of these commands connect to NATS. You need a configuration context that can be created using nats context, this command accept --context or will use the selected context.\nTo connect to Choria Brokers in Organization Issuer mode pass --choria-jwt and --choria-seed with your Choria tokens.\n  Searching Advisories When sampling the Replicator will publish advisories about node states. If you store these in a stream by ingesting choria.registration.advisories.\u003e you can search for a specific or just view all data:\n$ stream-replicator admin advisories REGISTRATION_ADVISORIES node1.example.net --since 5h Searching 17185 messages for advisories related to node1.example.net [2023-01-11 12:41:37] new node1.example.net seen 0s earlier on US_EAST_1 [2023-01-11 12:52:39] timeout node1.example.net seen 11m2s earlier on US_EAST_1 [2023-01-11 12:52:40] expire node1.example.net seen 11m3s earlier on US_EAST_1 Viewing the state The above advisories are built by tracking unique values seen in messages, you can view the state store:\n$ sudo stream-replicator admin state /var/lib/stream-replicator node1.example.net /var/lib/stream-replicator/CHORIA_REGISTRATION_US_EAST_1_4.json: Value: node1.example.net Seen Time: 2023-01-11 12:47:31.714555561 +0000 UTC (18s) Copied Time: 2023-01-11 12:47:31.721943055 +0000 UTC (18s) Payload Size: 13916 Advised: false Viewing cluster sync gossip When deploying the replicator in a cluster it will sync the state shown above using a gossip protocol, you can observe this in real time:\n$ stream-replicator admin gossip [CHORIA_REGISTRATION.US_EAST_1_4] size: 22272 advised: false: copied: 34m59.985s xx.example.net [CHORIA_REGISTRATION.US_EAST_1_3] size: 6567 advised: false: copied: 19m59.986s xx.example.net [CHORIA_REGISTRATION.US_EAST_1_2] size: 6668 advised: false: copied: 29m59.994s xx.example.net [CHORIA_REGISTRATION.US_EAST_1_2] size: 6748 advised: false: copied: 24m59.992s xx.example.net [CHORIA_REGISTRATION.US_EAST_1_3] size: 7119 advised: false: copied: 49m59.998s xx.example.net [CHORIA_REGISTRATION.US_EAST_1_3] size: 6748 advised: false: copied: 24m59.993s xx.example.net [CHORIA_REGISTRATION.US_EAST_1_3] size: 6785 advised: false: copied: 29m59.989s xx.example.net End to End latency monitoring To facilitate monitoring the latency from the point where a message was added to the source stream till it lands in the target one can look at the Choria-SR-Source header and compare it with the message time in the destination stream.\nBut what if you have a stream that is not seeing regular traffic? Stream Replicator can publish heartbeats into any subject to cause traffic to be produced thus facilitating continues monitoring, even on otherwise idle streams.\n Version Hint This requires version 0.8.0 of Stream Replicator\n  Heartbeats can be published to arbitrary subjects by adding the following configuration.\nheartbeats:  interval: 10s  url: nats://broker.choria.local:4222  leader_election: false   # tls:  # ca: /path/to/ca.pem  # cert: /path/to/cert.pem  # key: /path/to/key.pem   choria:  seed_file: /etc/stream-replicator/credentials/choria.seed  jwt_file: /etc/stream-replicator/credentials/choria.jwt  collective: choria   headers:  from: choria_compose   subjects:  - subject: choria.node_metadata._monitor  - subject: example._monitor  interval: 20s  headers:  noop: \"true\" Here we enable heartbeats to two subjects, the choria.node_metadata._monitor subject will get messages every 10 seconds with the from header added.\nThe example._monitor subject will get messages every 20 seconds and have both the from and noop headers.\nThe connection will be to a Choria Broker based on the choria configuration, an alternature traditional TLS connection is shown in addition.\nFor high availability one can enable leader_election where a specific replicator in a cluster of replicators will be elected as the one publishing metrics.\nThe messages being published will have a unix timestamp as body and headers Choria-SR-Originator indicating the host that published the heartbeat by hostname and Choria-SR-Subject indicating the subject it was published to.\nUsing nats CLI version 0.0.36 and newer can be used to monitor these messages arrive as planned:\n$ nats server check message --stream CHORIA_REGISTRATION --subject choria.node_metadata._monitor --body-timestamp OK Stream Message OK:Valid message on CHORIA_REGISTRATION \u003e choria.node_metadata._monitor | age=8.0830s size=10B The command has various flags for monitoring the age, see --help.\nPrometheus Data We have extensive Prometheus Metrics about the operation of the system allowing you to track message counts, size and efficiency of the Sampling feature.\n   Statistic Descriptions     choria_stream_replicator_tracker_total_items Number of entries being tracked for sampling purposes   choria_stream_replicator_tracker_seen_by_gossip Number of entries that we learned about via gossip synchronization   choria_stream_replicator_advisor_publish_errors The number of times publishing advisories failed   choria_stream_replicator_advisor_publish_total_messages The total number of advisories sent   choria_stream_replicator_limiter_messages_without_limit_field_count The number of messages that did not have the data field or header used for limiting/sampling   choria_stream_replicator_replicator_total_messages The total number of messages processed including ones that would be ignored   choria_stream_replicator_replicator_total_bytess The size of messages processed including ones that would be ignored   choria_stream_replicator_replicator_handler_error_count The number of times the handler failed to process a message   choria_stream_replicator_replicator_processing_time_seconds How long it took to process messages   choria_stream_replicator_replicator_stream_sequence The stream sequence of the last message received from the consumer   choria_stream_replicator_replicator_too_old_messages How many messages were discarded for being too old   choria_stream_replicator_replicator_copied_messages How many messages were copied   choria_stream_replicator_replicator_copied_bytes The size of messages that were copied   choria_stream_replicator_replicator_skipped_messages How many messages were skipped due to limiter configuration   choria_stream_replicator_replicator_skipped_bytes The size of messages that were skipped due to limited configuration   choria_stream_replicator_replicator_meta_parse_failed_count How many times a message metadata could not be parsed   choria_stream_replicator_replicator_ack_failed_count How many times an ack or nack failed   choria_stream_replicator_replicator_consumer_recreated How many times the source consumer had to be recreated   choria_stream_replicator_election_campaigns The number of campaigns a specific candidate voted in   choria_stream_replicator_election_leader Indicates if a specific instance is the current leader   choria_stream_replicator_election_interval_seconds The number of seconds between campaigns   choria_stream_replicator_heartbeat_subjects_count The number of subjects being published to   choria_stream_replicator_heartbeat_published_count The number of messags that was published   choria_stream_replicator_heartbeat_published_error_count The number of messags that failed to publish   choria_stream_replicator_heartbeat_publish_time Time taken for messages to be published including JetStream ACK time   choria_stream_replicator_heartbeat_paused Indicates heartbeat publishing is paused due to leader election    We have a published Grafana dashboard that you can install in your site, a screenshot of the dashboard is below.\n  \n","description":"","tags":null,"title":"Monitoring","uri":"/monitoring/index.html"},{"content":"We’ll show a full end to end walkthrough of building a centralised node metadata store for all your fleet nodes in multiple locations, including advisories about their availability.\nWhile the central store will get data for any single node hourly it will also get advisories letting it know when a node has not been seen for 11 minutes and when a node has not been seen for a hour allowing an up to date node view to be maintained. The in-datacenter data stores will have data no older than 5 minutes.\nData processing can be done in-datacenter and centrally using any of the 40+ languages NATS supports. One can even add a middle aggregation tier or complex tree like structures. Data cen be replicated once as here or multiple times to multiple locations and teams with different sampling strategies.\nTo achieve this we will use Choria Data Adapters to place data into Choria Streams and the replicator to move it to a central JetStream Cluster.\nThis is typically something you would do along with a Large Scale Choria Deployment spanning many locations and potentially millions of nodes.\n  \nAbove diagram demonstrates what we will construct:\n Choria Fleet Nodes publish their metadata every 300 seconds Choria Data Adapters place the data in the CHORIA_REGISTRATION stream with per-sender identifying information Stream Replicator reads all messages in the CHORIA_REGISTRATION Stream Sampling is applied and advisories are sent to the CHORIA_REGISTRATION_ADVISORIES stream about node movements and health Sampled Fleet Node metadata is replicated to central into the CHORIA_REGISTRATION stream All advisories are replicated to central into the CHORIA_REGISTRATION_ADVISORIES stream   Tip While not shown here these methods can be combined with the techniques in HA Clustering\n  Choria Streams Broker First we need to enable Choria Streams - an embedded, managed, version of NATS JetStream.\n# /etc/choria/broker.conf plugin.choria.network.stream.store = /var/lib/choria This stores our data in /var/lib/choria and sets up some basic streams. If the server is clustered it will form a RAFT-replicated Streams cluster.\nChoria Streams Data Adapter We now need to create a stream that will hold our in-datacenter data, we keep 5 most recent data items for any node and store any nodes data for no longer than a day.\n$ choria broker stream add CHORIA_REGISTRATION ? Subjects to consume choria.stream.input.registration.\u003e ? Storage backend file ? Retention Policy Limits ? Discard Policy Old ? Stream Messages Limit -1 ? Per Subject Messages Limit 5 ? Stream size limit -1 ? Maximum message age limit 1d ? Maximum individual message size -1 ? Duplicate tracking time window 2m0s ? Allow message Roll-ups No ? Allow message deletion Yes ? Allow purging subjects or the entire stream Yes ? Replicas 1 Stream CHORIA_REGISTRATION was created Information for Stream CHORIA_REGISTRATION created 2022-03-15T17:20:24+01:00 Configuration: Subjects: choria.stream.input.registration.\u003e Acknowledgements: true Retention: File - Limits Replicas: 1 Discard Policy: Old Duplicate Window: 2m0s Allows Msg Delete: true Allows Purge: true Allows Rollups: false Maximum Messages: unlimited Maximum Per Subject: 5 Maximum Bytes: unlimited Maximum Age: 1d0h0m0s Maximum Message Size: unlimited Maximum Consumers: unlimited State: Messages: 0 Bytes: 0 B FirstSeq: 0 LastSeq: 0 Active Consumers: 0 Here be sure to set Replicas to how many Choria Brokers you have in your cluster should you require high availability of this data.\nNote we set the subjects that the stream will ingest to choria.stream.input.registration.\u003e this means all subjects below that path, each nodes data will go into a unique subject.\nThe Data Adapter again goes into broker.conf:\n# /etc/choria/broker.conf plugin.choria.adapters = inventory plugin.choria.adapter.inventory.type = jetstream plugin.choria.adapter.inventory.stream.topic = choria.stream.input.registration.%s plugin.choria.adapter.inventory.ingest.topic = choria.broadcast.agent.registration plugin.choria.adapter.inventory.ingest.protocol = request Here we ingest data from choria.broadcast.agent.registration that will be in request format and place it into choria.stream.input.registration.%s where the %s gets replaced with the sender identity.\nFleet node configuration We now instruct every node in our fleet to send it’s data found about itself in /etc/choria/node-metadata.json to the network where the Data Adapters will receive, validate and transform it into registration data.\nregistration = inventory_content registration_splay = 120 registerinterval = 300 plugin.yaml = /etc/choria/node-metadata.json plugin.choria.registration.inventory_content.target = choria.broadcast.agent.registration plugin.choria.registration.inventory_content.compression = true To avoid huge DoS when large fleets start we introduce a 2 minute splay - each node will sleep a random period up to 2 minutes before starting their registration publishes. We publish this data every 5 minutes and compress it.\nPublished data will include lists of agents, facts, classes, node statistics and more, see registration.InventoryData.\nTarget Stream We should set up a Target in another NATS Cluster. The process is similar to the choria broker s add command above, but you can adjust retention to your liking, a day might be too short for example.\nAdvisories We’ll capture advisories and replicate those as well. Add streams as below:\n$ choria broker stream add CHORIA_REGISTRATION_ADVISORIES ? Subjects to consume choria.stream.input.registration_advisories ? Storage backend file ? Retention Policy Limits ? Discard Policy Old ? Stream Messages Limit -1 ? Per Subject Messages Limit -1 ? Stream size limit -1 ? Maximum message age limit 1w ? Maximum individual message size -1 ? Duplicate tracking time window 2m0s ? Allow message Roll-ups No ? Allow message deletion Yes ? Allow purging subjects or the entire stream Yes ? Replicas 1 Duplicate this stream in your target as well.\nStream Replicator Finally we are ready to set up the replication strategy.\nGiven that nodes publish their data every 300 seconds, we’ll set it to send timeout advisories after 11 minutes, we’ll sample hourly data and publish advisories. To allow for late generation of the node-metadata.json file, we trigger on a 1024 byte payload change as well.\nname: US-EAST monitor_port: 9100 loglevel: info state_store: /var/lib/stream-replicator logfile: /var/log/stream-replicator.log streams:  - stream: CHORIA_REGISTRATION  source_url: nats://choria.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  inspect_field: sender  inspect_duration: 1h  warn_duration: 11m  size_trigger: 1024  tls:  ca: /path/to/ca.pem  key: /path/to/key.pem  cert: /path/to/cert.pem  advisory:  subject: choria.stream.input.registration_advisories  reliable: true   - stream: CHORIA_REGISTRATION_ADVISORIES  source_url: nats://choria.us-east.example.net:4222  target_url: nats://nats.central.example.net:4222  tls:  ca: /path/to/ca.pem  key: /path/to/key.pem  cert: /path/to/cert.pem I am not going too deep into the tls settings, choria enroll --certname stream.replicator or similar can do the Choria side, you might need target_tls for the other side.\nWe copy both the CHORIA_REGISTRATION and CHORIA_REGISTRATION_ADVISORIES streams to central, the CHORIA_REGISTRATION_ADVISORIES is not sampled in any way.\nWhen you run the replicator data should start appearing centrally as well as advisories about node health.\n","description":"","tags":null,"title":"Choria Registration Data","uri":"/configuration/choria_registration/index.html"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/index.html"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/index.html"}]